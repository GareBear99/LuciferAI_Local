#!/usr/bin/env python3
"""
ðŸ§ª Complete Daemon Test - Local vs Consensus Fixes
Tests all daemon functionality and compares local vs global consensus fixes
"""
import os
import sys
import time
import hashlib
import json
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent / "core"))

from lucifer_watcher import LuciferWatcher
from relevance_dictionary import RelevanceDictionary
from lucifer_logger import LuciferLogger

PURPLE = "\033[35m"
GREEN = "\033[32m"
RED = "\033[31m"
GOLD = "\033[33m"
BLUE = "\033[34m"
CYAN = "\033[36m"
DIM = "\033[2m"
RESET = "\033[0m"

def get_user_id():
    import uuid
    device_id = str(uuid.UUID(int=uuid.getnode()))
    username = os.getenv("USER", "unknown")
    return hashlib.sha256(f"{device_id}-{username}".encode()).hexdigest()[:16].upper()

def create_test_scripts():
    """Create test scripts with various error types."""
    test_dir = Path.home() / "Desktop" / "daemon_test"
    test_dir.mkdir(exist_ok=True)
    
    print(f"{BLUE}Creating test scripts in: {test_dir}{RESET}\n")
    
    # Script 1: Import error (high quality - should be in consensus)
    script1 = test_dir / "test_high_quality.py"
    script1.write_text("""#!/usr/bin/env python3
# High quality fix candidate - common import error
data = json.dumps({"test": "value"})
print(data)
""")
    
    # Script 2: Import error (medium quality - local only)
    script2 = test_dir / "test_medium_quality.py"
    script2.write_text("""#!/usr/bin/env python3
# Medium quality - less common import
response = requests.get("https://api.github.com")
print(response.status_code)
""")
    
    # Script 3: Complex error (low quality - shouldn't qualify for consensus)
    script3 = test_dir / "test_low_quality.py"
    script3.write_text("""#!/usr/bin/env python3
# Low quality - specific to this project
from my_custom_module import special_function
result = special_function()
print(result)
""")
    
    # Script 4: Sequential errors
    script4 = test_dir / "test_sequential.py"
    script4.write_text("""#!/usr/bin/env python3
# Sequential errors - one fix leads to another
current_time = datetime.now()
formatted = current_time.strftime("%Y-%m-%d")
print(f"Date: {formatted}")
""")
    
    # Script 5: Multiple errors in one file
    script5 = test_dir / "test_multiple.py"
    script5.write_text("""#!/usr/bin/env python3
# Multiple errors
import os
file_path = Path("/tmp/test.txt")
data = json.load(open(file_path))
print(data)
""")
    
    return test_dir, [script1, script2, script3, script4, script5]


def setup_consensus_fixes(user_id):
    """Setup consensus (global) fixes - high quality only."""
    print(f"{PURPLE}{'='*70}{RESET}")
    print(f"{PURPLE}SETUP: Creating Consensus (Global) Fixes{RESET}")
    print(f"{PURPLE}{'='*70}{RESET}\n")
    
    lucifer_home = Path.home() / ".luciferai"
    fixnet_dir = lucifer_home / "fixnet"
    fixnet_dir.mkdir(parents=True, exist_ok=True)
    refs_file = fixnet_dir / "refs.json"
    
    # High quality consensus fixes (common errors)
    consensus_fixes = [
        {
            "fix_hash": hashlib.sha256(b"consensus-json-import").hexdigest()[:16],
            "user_id": "GLOBAL_USER_001",
            "error_type": "NameError",
            "timestamp": datetime.now().isoformat(),
            "script": "data_processing.py",
            "quality_score": 9.5,  # High quality
            "usage_count": 150,    # Widely used
            "success_rate": 0.98,  # Very successful
            "note": "Common JSON import fix"
        },
        {
            "fix_hash": hashlib.sha256(b"consensus-requests-import").hexdigest()[:16],
            "user_id": "GLOBAL_USER_002",
            "error_type": "NameError",
            "timestamp": datetime.now().isoformat(),
            "script": "api_client.py",
            "quality_score": 9.2,
            "usage_count": 200,
            "success_rate": 0.97,
            "note": "HTTP requests library import"
        },
        {
            "fix_hash": hashlib.sha256(b"consensus-datetime-import").hexdigest()[:16],
            "user_id": "GLOBAL_USER_003",
            "error_type": "NameError",
            "timestamp": datetime.now().isoformat(),
            "script": "time_utils.py",
            "quality_score": 9.8,
            "usage_count": 300,
            "success_rate": 0.99,
            "note": "Datetime module import"
        },
        {
            "fix_hash": hashlib.sha256(b"consensus-path-import").hexdigest()[:16],
            "user_id": "GLOBAL_USER_004",
            "error_type": "NameError",
            "timestamp": datetime.now().isoformat(),
            "script": "file_handler.py",
            "quality_score": 9.0,
            "usage_count": 120,
            "success_rate": 0.95,
            "note": "Path object import"
        }
    ]
    
    with open(refs_file, 'w') as f:
        json.dump(consensus_fixes, f, indent=2)
    
    print(f"{GREEN}âœ“ Created {len(consensus_fixes)} consensus fixes{RESET}")
    for fix in consensus_fixes:
        print(f"  â€¢ {fix['note']} (quality: {fix['quality_score']}, usage: {fix['usage_count']})")
    
    return consensus_fixes


def setup_local_fixes(rd: RelevanceDictionary):
    """Setup local fixes - including some that shouldn't go to consensus."""
    print(f"\n{PURPLE}{'='*70}{RESET}")
    print(f"{PURPLE}SETUP: Creating Local Fixes{RESET}")
    print(f"{PURPLE}{'='*70}{RESET}\n")
    
    local_fixes = [
        # High quality - should be considered for consensus
        {
            "error_type": "NameError",
            "error": "NameError: name 'json' is not defined",
            "solution": "import json",
            "quality": "HIGH",
            "usage_count": 50,
            "success_rate": 0.96
        },
        {
            "error_type": "NameError",
            "error": "NameError: name 'requests' is not defined",
            "solution": "import requests",
            "quality": "HIGH",
            "usage_count": 45,
            "success_rate": 0.94
        },
        
        # Medium quality - local only (less common)
        {
            "error_type": "NameError",
            "error": "NameError: name 'Path' is not defined",
            "solution": "from pathlib import Path",
            "quality": "MEDIUM",
            "usage_count": 15,
            "success_rate": 0.87
        },
        {
            "error_type": "ImportError",
            "error": "ImportError: cannot import name 'datetime' from 'datetime'",
            "solution": "from datetime import datetime",
            "quality": "MEDIUM",
            "usage_count": 20,
            "success_rate": 0.90
        },
        
        # Low quality - should NOT go to consensus (project-specific)
        {
            "error_type": "NameError",
            "error": "NameError: name 'my_custom_module' is not defined",
            "solution": "from my_custom_module import special_function",
            "quality": "LOW",
            "usage_count": 3,
            "success_rate": 0.67
        },
        {
            "error_type": "ImportError",
            "error": "ImportError: No module named 'very_specific_lib'",
            "solution": "import very_specific_lib",
            "quality": "LOW",
            "usage_count": 2,
            "success_rate": 0.50
        }
    ]
    
    added_fixes = []
    
    for fix in local_fixes:
        fix_content = f"{fix['error']}|{fix['solution']}"
        fix_hash = hashlib.sha256(fix_content.encode()).hexdigest()[:16]
        
        rd.add_fix(
            error_type=fix['error_type'],
            error_signature=fix['error'],
            solution=fix['solution'],
            fix_hash=fix_hash,
            context={
                "quality": fix['quality'],
                "usage_count": fix['usage_count'],
                "success_rate": fix['success_rate']
            }
        )
        
        # Simulate usage to update scores
        for _ in range(fix['usage_count']):
            success = (hash(fix_hash) % 100) / 100 < fix['success_rate']
            rd.record_fix_usage(fix_hash, success)
        
        added_fixes.append({
            "fix_hash": fix_hash,
            "quality": fix['quality'],
            "solution": fix['solution'][:50]
        })
    
    print(f"\n{GREEN}âœ“ Created {len(local_fixes)} local fixes{RESET}")
    
    # Categorize
    high_quality = [f for f in local_fixes if f['quality'] == 'HIGH']
    medium_quality = [f for f in local_fixes if f['quality'] == 'MEDIUM']
    low_quality = [f for f in local_fixes if f['quality'] == 'LOW']
    
    print(f"\n{CYAN}Quality Breakdown:{RESET}")
    print(f"  {GREEN}HIGH{RESET} (consensus candidates): {len(high_quality)}")
    print(f"  {GOLD}MEDIUM{RESET} (local only): {len(medium_quality)}")
    print(f"  {RED}LOW{RESET} (shouldn't sync): {len(low_quality)}")
    
    return added_fixes


def compare_fixes(rd: RelevanceDictionary):
    """Compare local vs consensus fixes."""
    print(f"\n{PURPLE}{'='*70}{RESET}")
    print(f"{PURPLE}COMPARISON: Local vs Consensus Fixes{RESET}")
    print(f"{PURPLE}{'='*70}{RESET}\n")
    
    # Sync with consensus
    rd.sync_with_remote()
    
    print(f"{CYAN}Fix Sources:{RESET}")
    print(f"  Local fixes: {len(rd.dictionary)}")
    print(f"  Consensus fixes: {len(rd.remote_refs)}")
    
    # Test queries
    test_queries = [
        ("json import", "NameError: name 'json' is not defined"),
        ("requests import", "NameError: name 'requests' is not defined"),
        ("Path import", "NameError: name 'Path' is not defined"),
        ("datetime import", "ImportError: cannot import name 'datetime'"),
        ("custom module", "NameError: name 'my_custom_module' is not defined")
    ]
    
    print(f"\n{CYAN}Comparing Fix Sources:{RESET}\n")
    
    for name, query in test_queries:
        print(f"{BLUE}Query: {name}{RESET}")
        matches = rd.search_similar_fixes(query, min_relevance=0.3)
        
        if matches:
            # Categorize by source
            local_matches = [m for m in matches if m['source'] == 'local']
            remote_matches = [m for m in matches if m['source'] == 'remote']
            
            print(f"  {GREEN}âœ“{RESET} Found {len(matches)} total")
            print(f"    Local: {len(local_matches)}")
            print(f"    Consensus: {len(remote_matches)}")
            
            if matches:
                best = matches[0]
                source_color = GREEN if best['source'] == 'local' else CYAN
                print(f"    Best: {source_color}{best['source']}{RESET} (score: {best['relevance_score']:.2f})")
        else:
            print(f"  {RED}âœ—{RESET} No matches")
        print()


def test_daemon_watch_mode(watcher: LuciferWatcher, test_dir: Path):
    """Test daemon in watch mode (suggest only)."""
    print(f"\n{PURPLE}{'='*70}{RESET}")
    print(f"{PURPLE}TEST: Daemon Watch Mode (Suggest Only){RESET}")
    print(f"{PURPLE}{'='*70}{RESET}\n")
    
    print(f"{GOLD}This mode suggests fixes without applying them{RESET}\n")
    
    # Add test directory to watcher
    watcher.add_path(str(test_dir))
    
    # Start in watch mode
    watcher.set_mode("watch")
    
    print(f"{GREEN}âœ“ Watcher configured in 'watch' mode{RESET}")
    print(f"{GREEN}âœ“ Monitoring: {test_dir}{RESET}")
    print(f"\n{DIM}(In real usage, watcher would run in background){RESET}\n")
    
    # Simulate detecting changes
    scripts = list(test_dir.glob("*.py"))
    print(f"{CYAN}Simulating file change detection...{RESET}\n")
    
    for script in scripts[:2]:  # Test first 2 scripts
        print(f"{BLUE}[Change Detected]{RESET} {script.name}")
        
        # Simulate what watcher would do
        error = watcher._detect_error(str(script))
        if error:
            print(f"  {GOLD}â†’{RESET} Error found: {error[:60]}...")
            error_type = watcher._classify_error(error)
            print(f"  {GOLD}â†’{RESET} Type: {error_type}")
            
            # Search for fix
            best_fix = watcher.dictionary.get_best_fix_for_error(error, error_type)
            if best_fix:
                source = best_fix.get('source', 'unknown')
                source_color = GREEN if source == 'local' else CYAN
                print(f"  {GREEN}âœ“{RESET} Found fix from {source_color}{source}{RESET}")
                print(f"  {GREEN}â†’{RESET} Solution: {best_fix['solution'][:60]}")
                print(f"  {GREEN}â†’{RESET} Score: {best_fix['relevance_score']:.2f}")
                print(f"  {GOLD}ðŸ’¡{RESET} Would suggest (not apply)")
            else:
                print(f"  {RED}âœ—{RESET} No fix found")
        else:
            print(f"  {GREEN}âœ“{RESET} No errors")
        print()


def test_daemon_autofix_mode(watcher: LuciferWatcher, test_dir: Path):
    """Test daemon in autofix mode (auto-apply)."""
    print(f"\n{PURPLE}{'='*70}{RESET}")
    print(f"{PURPLE}TEST: Daemon Autofix Mode (Auto-Apply){RESET}")
    print(f"{PURPLE}{'='*70}{RESET}\n")
    
    print(f"{GOLD}This mode automatically applies fixes from dictionary{RESET}\n")
    
    # Switch to autofix mode
    watcher.set_mode("autofix")
    
    print(f"{GREEN}âœ“ Watcher configured in 'autofix' mode{RESET}")
    print(f"\n{DIM}(In real usage, fixes would be auto-applied){RESET}\n")
    
    # Simulate auto-fixing
    scripts = list(test_dir.glob("*.py"))
    print(f"{CYAN}Simulating auto-fix on file changes...{RESET}\n")
    
    for script in scripts[2:4]:  # Test different scripts
        print(f"{BLUE}[Change Detected]{RESET} {script.name}")
        
        error = watcher._detect_error(str(script))
        if error:
            print(f"  {GOLD}â†’{RESET} Error: {error[:60]}...")
            error_type = watcher._classify_error(error)
            
            best_fix = watcher.dictionary.get_best_fix_for_error(error, error_type)
            if best_fix:
                source = best_fix.get('source', 'unknown')
                source_color = GREEN if source == 'local' else CYAN
                print(f"  {GREEN}âœ“{RESET} Found fix from {source_color}{source}{RESET}")
                print(f"  {GREEN}â†’{RESET} Applying: {best_fix['solution'][:60]}")
                
                # Simulate application
                success = best_fix['relevance_score'] > 0.7
                if success:
                    print(f"  {GREEN}âœ…{RESET} Fix applied successfully")
                    print(f"  {GREEN}â†’{RESET} Script would now run")
                else:
                    print(f"  {RED}âŒ{RESET} Fix failed, would revert")
            else:
                print(f"  {RED}âœ—{RESET} No fix available")
        else:
            print(f"  {GREEN}âœ“{RESET} No errors")
        print()


def test_fix_qualification():
    """Test which fixes qualify for consensus."""
    print(f"\n{PURPLE}{'='*70}{RESET}")
    print(f"{PURPLE}ANALYSIS: Fix Qualification for Consensus{RESET}")
    print(f"{PURPLE}{'='*70}{RESET}\n")
    
    print(f"{CYAN}Consensus Qualification Criteria:{RESET}")
    print(f"  â€¢ Quality score: â‰¥ 8.0")
    print(f"  â€¢ Usage count: â‰¥ 25")
    print(f"  â€¢ Success rate: â‰¥ 0.85")
    print(f"  â€¢ Not project-specific")
    print()
    
    # Test fixes
    test_fixes = [
        {"name": "JSON import", "quality": 9.5, "usage": 50, "success": 0.96, "specific": False},
        {"name": "requests import", "quality": 9.2, "usage": 45, "success": 0.94, "specific": False},
        {"name": "Path import", "quality": 7.5, "usage": 15, "success": 0.87, "specific": False},
        {"name": "datetime import", "quality": 8.5, "usage": 20, "success": 0.90, "specific": False},
        {"name": "custom module", "quality": 5.0, "usage": 3, "success": 0.67, "specific": True},
        {"name": "specific lib", "quality": 4.5, "usage": 2, "success": 0.50, "specific": True},
    ]
    
    print(f"{CYAN}Fix Evaluation:{RESET}\n")
    
    qualified = []
    not_qualified = []
    
    for fix in test_fixes:
        qualifies = (
            fix['quality'] >= 8.0 and
            fix['usage'] >= 25 and
            fix['success'] >= 0.85 and
            not fix['specific']
        )
        
        status = f"{GREEN}âœ“ QUALIFIES{RESET}" if qualifies else f"{RED}âœ— LOCAL ONLY{RESET}"
        print(f"{fix['name']:20} {status}")
        print(f"  Quality: {fix['quality']:.1f}/10  Usage: {fix['usage']}  Success: {fix['success']:.0%}  Specific: {fix['specific']}")
        
        if qualifies:
            qualified.append(fix)
        else:
            not_qualified.append(fix)
        print()
    
    print(f"\n{GREEN}Consensus Candidates: {len(qualified)}{RESET}")
    print(f"{GOLD}Local Only: {len(not_qualified)}{RESET}")


def print_summary():
    """Print test summary."""
    print(f"\n{PURPLE}{'='*70}{RESET}")
    print(f"{PURPLE}TEST SUMMARY{RESET}")
    print(f"{PURPLE}{'='*70}{RESET}\n")
    
    print(f"{GREEN}âœ… Tests Completed:{RESET}")
    print(f"  â€¢ Consensus fixes setup")
    print(f"  â€¢ Local fixes setup (3 quality levels)")
    print(f"  â€¢ Local vs Consensus comparison")
    print(f"  â€¢ Daemon watch mode (suggest)")
    print(f"  â€¢ Daemon autofix mode (apply)")
    print(f"  â€¢ Fix qualification analysis")
    print()
    
    print(f"{CYAN}Key Findings:{RESET}")
    print(f"  â€¢ {GREEN}HIGH quality{RESET} local fixes match consensus quality")
    print(f"  â€¢ {GOLD}MEDIUM quality{RESET} fixes useful locally but not for consensus")
    print(f"  â€¢ {RED}LOW quality{RESET} fixes stay local (project-specific)")
    print(f"  â€¢ Both modes successfully find and suggest/apply fixes")
    print(f"  â€¢ Consensus fixes have higher usage/success metrics")
    print()
    
    print(f"{BLUE}Daemon Modes:{RESET}")
    print(f"  â€¢ {GREEN}Watch Mode:{RESET} Suggests fixes for review (development)")
    print(f"  â€¢ {CYAN}Autofix Mode:{RESET} Auto-applies fixes (servers/production)")
    print()
    
    print(f"{PURPLE}All daemon tests complete! ðŸ©¸âœ¨{RESET}\n")


def main():
    print(f"\n{PURPLE}{'='*80}{RESET}")
    print(f"{PURPLE}{'ðŸ§ª COMPLETE DAEMON TEST - LOCAL VS CONSENSUS':^80}{RESET}")
    print(f"{PURPLE}{'='*80}{RESET}\n")
    
    # Initialize
    user_id = get_user_id()
    print(f"{GOLD}User ID: {user_id}{RESET}\n")
    
    rd = RelevanceDictionary(user_id)
    logger = LuciferLogger()
    watcher = LuciferWatcher(user_id)
    
    # Setup
    consensus_fixes = setup_consensus_fixes(user_id)
    local_fixes = setup_local_fixes(rd)
    test_dir, scripts = create_test_scripts()
    
    # Compare
    compare_fixes(rd)
    
    # Test daemon modes
    test_daemon_watch_mode(watcher, test_dir)
    test_daemon_autofix_mode(watcher, test_dir)
    
    # Analysis
    test_fix_qualification()
    
    # Summary
    print_summary()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print(f"\n\n{GOLD}Test interrupted{RESET}\n")
    except Exception as e:
        print(f"\n\n{RED}Error: {e}{RESET}\n")
        import traceback
        traceback.print_exc()
