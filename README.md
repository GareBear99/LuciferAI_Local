# ðŸ‘¾ LuciferAI Local - Warp AI Clone

A **local, open-source terminal AI assistant** modeled after Warp AI. Execute commands, read/write files, search codebases, and moreâ€”all from an interactive terminal interface.

## ðŸŒŸ Features

### âœ… Phase 1 Complete
- **File Operations**: Read, write, edit, find files âœ…
- **Code Search**: Grep-style search across directories âœ…
- **Command Execution**: Run shell commands safely with risk detection âœ…
- **Directory Listing**: Browse filesystem with metadata âœ…
- **Environment Info**: Get current directory, user, shell info âœ…
- **Interactive CLI**: Clean, colorful terminal interface âœ…

### âœ… Phase 2 Complete - FixNet & Self-Healing
- **Authentication**: AES-256 encryption with device binding âœ…
- **Auto-Fix**: Automatic error detection and repair âœ…
- **FixNet Upload**: Encrypted fixes to public GitHub repo âœ…
- **Relevance Dictionary**: Collaborative learning from all users âœ…
- **Branch Tracking**: Links between related fixes âœ…
- **Remote Search**: Find fixes from other users âœ…

### ðŸ”® Phase 3 - Coming Soon
- **AI Model Integration**: Mistral, OpenAI, Claude, Ollama support
- **Conversation Memory**: Multi-turn conversations with context
- **Code Analysis**: AST-based code understanding

## ðŸš€ Quick Start

### Installation

```bash
cd ~/Desktop/Projects/LuciferAI_Local

# Install dependencies (no AI models yet for testing)
pip3 install rich colorama prompt_toolkit watchdog pathspec

# Make executable
chmod +x lucifer.py

# Run
./lucifer.py
```

### Usage Examples

```
You > help
ðŸ‘¾ LuciferAI Capabilities...

You > read config.yaml
ðŸ” Reading file: config.yaml
âœ… Read config.yaml (50 lines, 1234 bytes)
[file contents...]

You > find *.py
ðŸ” Finding files: *.py
âœ… Found 5 Python files matching '*.py':
  ðŸ“„ lucifer.py
  ðŸ“„ core/agent.py
  ðŸ“„ tools/file_tools.py
  ...

You > run git status
âš¡ Running command: git status
âœ… Command executed successfully:
On branch main
nothing to commit, working tree clean

You > list .
ðŸ“‚ Listing directory: .
âœ… Contents of /Users/.../LuciferAI_Local:
  ðŸ“ core
  ðŸ“ tools
  ðŸ“„ lucifer.py (2453 bytes)
  ...

You > exit
ðŸ‘‹ Farewell, mortal. LuciferAI signing off.
```

## ðŸ“ Project Structure

```
LuciferAI_Local/
â”œâ”€â”€ lucifer.py              # Main CLI entry point
â”œâ”€â”€ core/
â”‚   â””â”€â”€ agent.py            # Agent orchestrator & request router
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ file_tools.py       # File operations (read/write/search)
â”‚   â””â”€â”€ command_tools.py    # Command execution & environment
â”œâ”€â”€ logs/                   # Conversation logs (future)
â”œâ”€â”€ tests/                  # Test suite
â””â”€â”€ requirements.txt        # Python dependencies
```

## ðŸ”§ Tool Functions

### File Tools (`tools/file_tools.py`)
- `read_file(path, line_range)` - Read files with optional line range
- `write_file(path, content)` - Write/create files
- `edit_file(path, search, replace)` - Search and replace in files
- `find_files(pattern, search_dir)` - Find files by pattern
- `grep_search(query, path)` - Search text in files
- `list_directory(path)` - List directory contents

### Command Tools (`tools/command_tools.py`)
- `run_command(command, cwd, timeout)` - Execute shell commands
- `run_python_code(code)` - Run Python code safely
- `get_env_info()` - Get environment information
- `check_command_exists(command)` - Check if command is available
- `is_risky_command(command)` - Detect dangerous commands

## ðŸ§ª Testing

Each module has built-in tests:

```bash
# Test file tools
cd tools && python3 file_tools.py

# Test command tools
cd tools && python3 command_tools.py

# Test agent
cd core && python3 agent.py
```

## ðŸ”’ Safety Features

- **Risky Command Detection**: Blocks dangerous commands (rm -rf, dd, etc.)
- **Timeout Protection**: Commands auto-timeout after 30s
- **Sandboxed Python**: Python code runs in subprocess
- **Path Validation**: All file paths are validated
- **Error Handling**: Comprehensive try/catch blocks

## ðŸŽ¨ Current Workflow

```
User Input â†’ Agent.process_request()
            â†“
    Parse Intent (regex-based)
            â†“
    Route to Handler (_handle_read_file, etc.)
            â†“
    Call Tool Function (read_file, run_command, etc.)
            â†“
    Format Response
            â†“
    Return to User
```

## ðŸš§ Roadmap

### Phase 1: Core Tools âœ… (Complete)
- [x] File operations
- [x] Command execution
- [x] Search functionality
- [x] Interactive CLI
- [x] Safety checks

### Phase 2: AI Integration (Next)
- [ ] Add Mistral AI client
- [ ] Implement conversation memory
- [ ] Add streaming responses
- [ ] Create AI model abstraction layer
- [ ] Support OpenAI, Claude, Ollama

### Phase 3: Advanced Features
- [ ] RAG with ChromaDB/FAISS
- [ ] Code analysis (AST parsing)
- [ ] Auto-fix suggestions
- [ ] File watcher daemon
- [ ] Git integration
- [ ] Multi-step planning

### Phase 4: GUI & Distribution
- [ ] PyQt5 GUI (optional)
- [ ] Build macOS .app
- [ ] Cross-platform packaging
- [ ] Plugin system

## ðŸ¤ How to Add AI Models

### Option 1: Mistral AI (Recommended)

```bash
# Install
pip install mistralai

# Set API key
export MISTRAL_API_KEY="your-key-here"

# Usage in agent.py (coming soon)
from mistralai.client import MistralClient

client = MistralClient(api_key=os.getenv("MISTRAL_API_KEY"))
response = client.chat(
    model="mistral-large-latest",
    messages=[{"role": "user", "content": user_input}]
)
```

### Option 2: Ollama (Local, Free)

```bash
# Install Ollama
brew install ollama

# Download model
ollama pull codellama

# Usage (coming soon)
import ollama
response = ollama.chat(
    model="codellama",
    messages=[{"role": "user", "content": user_input}]
)
```

### Option 3: OpenAI

```bash
pip install openai
export OPENAI_API_KEY="your-key-here"
```

## ðŸ“ Contributing

This is a personal project, but feel free to fork and customize!

## âš–ï¸ License

MIT License - Do whatever you want with it!

## ðŸŽ¯ Design Philosophy

**Why not just use Warp AI?**
- Learn how agentic systems work
- Full control over data and privacy
- Customize for specific workflows
- Integrate with local tools
- No API costs with local models

**Inspired by:**
- Warp AI
- GitHub Copilot
- Cursor
- Aider

## ðŸ©¸ The LuciferAI Way

> "Born in Neon. Forged in Silence."

Purple theme, skull emojis, and a rebellious attitude. Because AI assistants don't have to be boring.

---

**Status**: Phase 1 Complete âœ… | Phase 2 In Progress ðŸš§

Made with ðŸ©¸ by TheRustySpoon
