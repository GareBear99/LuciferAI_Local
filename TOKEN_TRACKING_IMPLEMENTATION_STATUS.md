# Token Tracking Implementation Status

## âœ… COMPLETED IMPLEMENTATIONS

### 1. **LLM Token Tracking**
**Status:** Fully implemented

**Changes Made:**
- `/core/llm_backend.py` - Updated `chat()` method to handle `return_stats` parameter
- `/core/enhanced_agent.py` - Added `return_stats=True` to all LLM calls:
  - Line 8060: General LLM queries
  - Line 8978: Multi-step script planning
  - Line 9384: Code generation
  - Line 10153: Execution summaries

**Display Format:**
```
ğŸ’¬ mistral-7b:
[Response text here]
   [Input: 54 tokens (267 chars), Output: 23 tokens (92 chars), Total: 77 tokens]
```

### 2. **Fallback Parser Token-Equivalent Tracking**
**Status:** Just implemented

**Location:** `/core/enhanced_agent.py` lines 9069-9083

**Metrics Tracked:**
- Input characters and words
- Output characters and words  
- Estimated token counts (4 chars per token, or 0.75 words per token)

**Display Format:**
```
âš ï¸  All LLM models failed or timed out
ğŸ”„ Using dynamic fallback parser for step generation

ğŸ“‹ Task Checklist (generated by parser):
   [Parser: 12 tokens in (48 chars, 9 words), 35 tokens out (140 chars, 26 words), 47 total]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### 3. **Step Display with Separator Lines**
**Status:** Fully implemented

**Location:** `/core/lucifer_colors.py` lines 764-768

**Display Format:**
```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ Step 1/3: Creating file and setting up structure

[task execution here]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ Step 2/3: Writing code implementation

[task execution here]
```

### 4. **'build' Keyword Support**
**Status:** Implemented

**Location:** `/core/enhanced_agent.py` line 8733

Added "build" to script creation detection keywords.

### 5. **Model Files Location**
**Status:** Fixed

All model files moved from `.luciferai/models/` and `~/Desktop/models/` to `/models/` directory.

---

## âŒ IDENTIFIED ISSUES & REQUIRED FIXES

### **Issue #1: Model Timeouts (CRITICAL)**

**Problem:** All models timing out after 30s during initialization/inference on Catalina

**Evidence:**
```
mistral-7b (Tier 2) timed out after 30s
gemma2 (Tier 1) failed: Llamafile error
tinyllama (Tier 0) timed out after 30s
phi-2 (Tier 0) timed out after 30s
```

**Root Cause:**
- Older macOS (Catalina) has slower llamafile initialization
- 30s timeout insufficient for model load + inference
- No model caching between requests

**Fix Required:**
```python
# File: /core/llm_backend.py
# Location: Line 44-52

self.tier_timeouts = {
    'tinyllama': 60,         # Was 20 â†’ Increase to 60s for Catalina
    'llama3.2': 90,          # Was 30 â†’ Increase to 90s
    'mistral': 120,          # Was 45 â†’ Increase to 120s
    'deepseek-coder': 180,   # Was 60 â†’ Increase to 180s
    'llama3.1-70b': 180,     # Was 120 â†’ Increase to 180s
    'mixtral-8x22b': 240,    # Was 120 â†’ Increase to 240s
    'qwen-72b': 240          # Was 120 â†’ Increase to 240s
}
```

**Additional Fix - Model Caching:**
```python
# File: /core/llm_backend.py  
# Add after line 97

def _check_native_llamafile(self) -> bool:
    """Check if native llamafile binary exists."""
    llamafile_path = LUCIFER_HOME / 'bin' / 'llamafile'
    if llamafile_path.exists():
        # Pre-warm model cache on first use
        self._prewarm_model_cache()
        return True
    return False

def _prewarm_model_cache(self):
    """Pre-load model into memory to avoid cold-start delays."""
    # Implementation: Run a quick test inference to warm cache
    pass
```

---

### **Issue #2: No Bypass Routing Display**

**Problem:** Bypass routing messages don't show even with multiple models

**Root Cause:** Code only shows bypass when there are LOWER tier models to skip

**Current Logic** (line 8822-8827):
```python
if unique_lower_models:  # Only shows if lower tiers exist
    skipped_parts = [...]
    print(c(f"ğŸ’¡ Bypassed: ", "dim") + ", ".join(skipped_parts))
```

**Fix Required:**
```python
# File: /core/enhanced_agent.py
# Location: Lines 8800-8832

if best_model:
    # Show which model we're using (ALWAYS show routing)
    print(c(f"ğŸ§  Routing to: {best_model}", "cyan") + c(f" ({tier_names[best_tier]})", "dim"))
    
    # Show bypass info if any models were skipped
    if unique_lower_models:
        skipped_str = ", ".join([c(f"{model} ({tier_names[get_model_tier(model)]})", "yellow") for model in unique_lower_models])
        print(c(f"   Bypassed: {skipped_str}", "dim"))
    
    print()
    sys.stdout.flush()
```

**Apply to all workflow entry points:**
- Line 8806: `_handle_single_task_with_llm()`
- Line 8885: `_handle_multi_step_script_creation()`
- Line 10152: `_handle_find_and_write_workflow()`

---

### **Issue #3: Execution Statistics Not Displaying**

**Problem:** The `execution_tracker.py` module exists but isn't integrated

**Evidence:**
- `format_stats_display()` function exists (lines 306-474)
- Never called at end of workflows
- No ExecutionTracker instance created

**Fix Required:**

**Step 1:** Initialize tracker in `EnhancedLuciferAgent.__init__()`:
```python
# File: /core/enhanced_agent.py
# Add after line 225

from core.execution_tracker import ExecutionTracker
self.execution_tracker = ExecutionTracker()
```

**Step 2:** Track operations throughout workflows:
```python
# When files created:
self.execution_tracker.track_file_created(file_path)

# When models used:
self.execution_tracker.track_model_used(model_name, tier, purpose='code_generation', tokens=token_count)

# When templates used:
self.execution_tracker.track_template_used(template_name, relevance_score)
```

**Step 3:** Display stats at end of workflows:
```python
# File: /core/enhanced_agent.py
# Add before final return in:
# - _handle_multi_step_script_creation() (line 10185)
# - _handle_find_and_write_workflow() (line ~11000)

# End execution tracking
self.execution_tracker.end_execution()

# Display execution statistics
print()
print(self.execution_tracker.format_stats_display())
print()
```

---

### **Issue #4: Token Display Format Inconsistency**

**Problem:** LLM tokens and parser tokens have different formats

**Current State:**
- LLM: `[Input: X tokens (Y chars), Output: Z tokens (W chars), Total: T tokens]`
- Parser: `[Parser: X tokens in (Y chars, Z words), W tokens out (A chars, B words), T total]`

**Fix Required:**
```python
# File: /core/enhanced_agent.py
# Location: Line 9083

# BEFORE:
print(c(f"   [Parser: {input_tokens} tokens in ({input_chars} chars, {input_words} words), {output_tokens} tokens out ({output_chars} chars, {output_words} words), {total_tokens} total]", "dim"))

# AFTER (match LLM format):
print(c(f"   [Input: {input_tokens} tokens ({input_chars} chars), Output: {output_tokens} tokens ({output_chars} chars), Total: {total_tokens} tokens]", "dim"))
print(c(f"   [Method: Dynamic parser (rule-based, no LLM)]", "dim"))
```

---

### **Issue #5: Missing Token Display on All LLM Responses**

**Problem:** Token stats only show when `return_stats` works, but some LLM calls might not return stats

**Locations to verify:**
- Line 8816: `_get_llm_acknowledgment()` - Check if uses token tracking
- Line 8832: `_get_llm_task_commentary()` - Check if uses token tracking
- Line 9269-9311: Template validation - Check if uses token tracking

**Fix Required:** Add `return_stats=True` to any missing LLM calls and display logic

---

## ğŸ“Š TESTING CHECKLIST

To verify all implementations work:

```bash
cd /Users/TheRustySpoon/Desktop/Projects/LuciferAI_Local
python lucifer.py
```

Then test: `build me a script that prints hello world`

**Expected Output:**
```
ğŸ’¡ Routing to: mistral-7b (Tier 2)
   Bypassed: tinyllama (Tier 0), phi-2 (Tier 0), gemma2 (Tier 1)

ğŸ¤” mistral-7b (Tier 2) thinking: [streaming response]
   [Input: 54 tokens (267 chars), Output: 23 tokens (92 chars), Total: 77 tokens]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“‹ mistral-7b - Task Checklist:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  [ ] 1. Create file structure
  [ ] 2. Write implementation code
  [ ] 3. Test script execution
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ Step 1/3: Create file structure

[execution here]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ Step 2/3: Write implementation code

ğŸ§  Routed to: mistral-7b (Tier 2)
[code generation]
   [Input: 89 tokens (445 chars), Output: 42 tokens (168 chars), Total: 131 tokens]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ Step 3/3: Test script execution

[test execution]

ğŸ“Š Execution Statistics:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ Files affected: 1
   â€¢ Created: 1
     - hello_world.py
ğŸ§  Models used: 1
   â€¢ mistral-7b (Tier 2) - code_generation, planning [154 tokens]
â±ï¸  Execution time: 45.23s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ’¬ mistral-7b - Execution Summary:
Created a Python script that prints 'Hello World' to demonstrate basic output.
   [Input: 89 tokens (445 chars), Output: 19 tokens (76 chars), Total: 108 tokens]
```

---

## ğŸ¯ PRIORITY ORDER

1. **CRITICAL:** Fix model timeouts (Issue #1) - Blocks all functionality
2. **HIGH:** Consistent token display format (Issue #4) - User confusion
3. **MEDIUM:** Always show routing (Issue #2) - Better transparency
4. **MEDIUM:** Execution statistics (Issue #3) - Nice to have
5. **LOW:** Verify all LLM calls have token tracking (Issue #5) - Already mostly done
