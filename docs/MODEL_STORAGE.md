# Model Storage Structure

Complete guide to LuciferAI's model storage locations and organization.

---

## Overview

LuciferAI uses multiple storage locations for AI models to support different use cases:
- **Project models** - Portable, version-controlled models in project directory
- **Bundled models** - Pre-installed models in `.luciferai` home directory
- **Custom models** - User-added GGUF models in project directory

---

## Storage Locations

### 1. Project Models Directory

**Location**: `models/` (relative to project root)

**Purpose**: Portable models that travel with your project

**Structure**:
```
LuciferAI_Local/
└── models/
    ├── custom_models/              # User-added GGUF models
    │   ├── qwen2-7b.gguf
    │   ├── my-custom-model.gguf
    │   └── ...
    └── (placeholder for future project-specific models)
```

**Characteristics**:
- ✅ **Portable** - Models stay with project when you move/copy it
- ✅ **Version controlled** - Can be committed to git (if small enough)
- ✅ **Per-project** - Different projects can have different models
- ❌ **No automatic downloads** - Must manually add models here

**Use Case**: Custom GGUF models you want to bundle with your project

---

### 2. Bundled Models Directory

**Location**: `~/.luciferai/models/` (in user home directory)

**Purpose**: System-wide models shared across all LuciferAI instances

**Structure**:
```
~/.luciferai/
└── models/
    ├── tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf    # Pre-bundled
    ├── phi-2/                                    # Ollama models
    ├── mistral/
    ├── gemma2/
    └── deepseek-coder/
```

**Characteristics**:
- ✅ **Shared** - One copy used by all projects
- ✅ **Managed by Ollama** - Automatic downloads and updates
- ✅ **Pre-installed** - TinyLlama comes bundled
- ✅ **Space efficient** - No duplication across projects
- ❌ **Not portable** - Doesn't move with project

**Use Case**: General-purpose models used across multiple projects

---

### 3. Custom Models Subdirectory

**Location**: `models/custom_models/` (project-specific)

**Purpose**: User-added GGUF model files

**How It Works**:
1. Download `.gguf` model file from HuggingFace or other source
2. Place in `models/custom_models/` directory
3. LuciferAI auto-detects on startup
4. Enable with `llm enable <model-name>`

**Example**:
```bash
# Download model
cd models/custom_models
wget https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q4_k_m.gguf

# Restart LuciferAI (auto-detects new model)
python3 lucifer.py

# Enable the model
> llm enable qwen2-7b-instruct-q4_k_m
> llm list
```

**Supported Files**:
- `.gguf` files only
- Any quantization (Q2_K, Q3_K_M, Q4_K_M, Q5_K_M, Q8_0)
- No directory structure required (flat files)

---

### 4. Image Models Directory

**Location**: `~/.luciferai/image_models/`

**Purpose**: Image generation models (Flux, Stable Diffusion, etc.)

**Structure**:
```
~/.luciferai/
└── image_models/
    ├── flux.1-schnell/
    └── stable-diffusion-1.5/
```

**Characteristics**:
- System-wide storage
- Managed by image generation plugin
- Separate from LLM models

**See Also**: [CUSTOM_INTEGRATIONS.md](CUSTOM_INTEGRATIONS.md) for image generation setup

---

### 5. Image Cache Directory

**Location**: `~/.luciferai/images/`

**Purpose**: Google Images search cache and downloaded images

**Structure**:
```
~/.luciferai/
└── images/
    ├── image_cache.json      # Search results cache
    ├── 8a9f3b12.jpg         # Downloaded images
    ├── c4e2d5a1.png
    └── ...
```

**See Also**: [CUSTOM_INTEGRATIONS.md](CUSTOM_INTEGRATIONS.md) for image retrieval

---

### 6. Generated Images Directory

**Location**: `~/.luciferai/generated_images/`

**Purpose**: AI-generated images from text prompts

**Structure**:
```
~/.luciferai/
└── generated_images/
    ├── image_001.png
    ├── image_002.png
    └── ...
```

**Generated by**: `generate image <prompt>` command

---

## Storage Decision Tree

**Where should I put my model?**

```
START
  │
  ├─ Is it a GGUF file?
  │   ├─ YES → Place in models/custom_models/
  │   └─ NO → Continue
  │
  ├─ Is it an image generation model?
  │   ├─ YES → Install to ~/.luciferai/image_models/
  │   └─ NO → Continue
  │
  ├─ Is it managed by Ollama?
  │   ├─ YES → Use `install <model-name>` command
  │   │         (auto-installs to ~/.luciferai/models/)
  │   └─ NO → See CUSTOM_INTEGRATIONS.md for plugins
```

---

## Directory Creation

All directories are **auto-created** on first use:

- `models/custom_models/` - Created by enhanced_agent.py on startup
- `~/.luciferai/models/` - Created when bundled models are installed
- `~/.luciferai/image_models/` - Created by image_generator.py
- `~/.luciferai/images/` - Created by image_retrieval.py
- `~/.luciferai/generated_images/` - Created when first image is generated

**Manual Creation** (optional):
```bash
# Create all directories at once
mkdir -p models/custom_models
mkdir -p ~/.luciferai/{models,image_models,images,generated_images}
```

---

## Storage Requirements

### Disk Space by Model Type

| Model Type | Size Range | Location | Example |
|------------|------------|----------|---------|
| **Tier 0** (1-3B) | 600MB - 2GB | `~/.luciferai/models/` | TinyLlama (600MB) |
| **Tier 1** (3-8B) | 2GB - 5GB | `~/.luciferai/models/` | Gemma2 (4.8GB) |
| **Tier 2** (7-13B) | 4GB - 8GB | `~/.luciferai/models/` | Mistral (4.1GB) |
| **Tier 3** (13B+) | 4GB - 20GB | `~/.luciferai/models/` | DeepSeek 33B (19GB) |
| **Tier 4** (70B+) | 40GB - 100GB | `~/.luciferai/models/` | Llama3.1-70B (70GB) |
| **Custom GGUF** | Varies | `models/custom_models/` | User models |
| **Image Models** | 2GB - 8GB | `~/.luciferai/image_models/` | Flux (4GB) |

### Total Space Planning

**Minimal Setup** (~1GB):
- TinyLlama (600MB) - Bundled

**Recommended Setup** (~15GB):
- TinyLlama (600MB) - Tier 0
- Phi-2 (1.7GB) - Tier 0
- Gemma2 (4.8GB) - Tier 1
- Mistral (4.1GB) - Tier 2
- DeepSeek (3.8GB) - Tier 3

**Full Setup** (~450GB):
- All 85+ supported models
- Use external drive if needed

---

## Cleanup and Maintenance

### Remove Unused Models

**Ollama models** (in `~/.luciferai/models/`):
```bash
# List models
ollama list

# Remove specific model
ollama rm mistral

# Or use LuciferAI
python3 lucifer.py -c "uninstall mistral"
```

**Custom models** (in `models/custom_models/`):
```bash
# Simply delete the .gguf file
rm models/custom_models/old-model.gguf

# Restart LuciferAI to update detection
python3 lucifer.py
```

### Check Disk Usage

```bash
# Check bundled models
du -sh ~/.luciferai/models/

# Check custom models
du -sh models/custom_models/

# Check image files
du -sh ~/.luciferai/images/
du -sh ~/.luciferai/image_models/
du -sh ~/.luciferai/generated_images/

# Total LuciferAI storage
du -sh ~/.luciferai/
```

### Clear Image Cache

```bash
# Clear downloaded images
rm -rf ~/.luciferai/images/*.{jpg,png,gif,webp}

# Clear image cache metadata
rm ~/.luciferai/images/image_cache.json

# Clear generated images
rm -rf ~/.luciferai/generated_images/*.png
```

---

## Backup Strategy

### What to Backup

**Essential** (always backup):
- `models/custom_models/` - Your custom GGUF models
- `.luciferai_ids` - Encrypted ID mappings (if using GitHub features)
- `~/.luciferai/data/` - User data and settings

**Optional** (can re-download):
- `~/.luciferai/models/` - Ollama models (can reinstall)
- `~/.luciferai/image_models/` - Image models (can reinstall)

**Don't Backup** (cached/temporary):
- `~/.luciferai/images/` - Image cache (temporary)
- `~/.luciferai/generated_images/` - Generated images (unless you want them)

### Backup Commands

```bash
# Backup custom models
tar -czf luciferai-custom-models-$(date +%Y%m%d).tar.gz models/custom_models/

# Backup user data
tar -czf luciferai-data-$(date +%Y%m%d).tar.gz ~/.luciferai/data/

# Full backup (excluding Ollama models to save space)
tar -czf luciferai-backup-$(date +%Y%m%d).tar.gz \
  models/custom_models/ \
  ~/.luciferai/data/ \
  --exclude='~/.luciferai/models/*' \
  --exclude='~/.luciferai/images/*'
```

---

## Migration and Portability

### Moving Project to New Machine

**Option 1: Without Models** (faster, smaller transfer)
```bash
# On old machine - exclude models
rsync -av --exclude='models/' --exclude='.luciferai/' \
  LuciferAI_Local/ newmachine:LuciferAI_Local/

# On new machine - reinstall models
cd LuciferAI_Local
python3 lucifer.py -c "install core models"
```

**Option 2: With Custom Models** (portable, complete)
```bash
# On old machine - include custom models only
rsync -av --exclude='.luciferai/' \
  LuciferAI_Local/ newmachine:LuciferAI_Local/

# Custom models in models/custom_models/ travel with project
```

### Moving Bundled Models

```bash
# On old machine - export bundled models
tar -czf ollama-models.tar.gz ~/.luciferai/models/

# Transfer to new machine
scp ollama-models.tar.gz newmachine:~/

# On new machine - import
mkdir -p ~/.luciferai/
tar -xzf ~/ollama-models.tar.gz -C ~/.luciferai/
```

---

## Symlinks and Custom Locations

### Use External Drive for Models

**If you need to store models on external drive:**

```bash
# Move models to external drive
mv ~/.luciferai/models /Volumes/ExternalDrive/luciferai-models

# Create symlink
ln -s /Volumes/ExternalDrive/luciferai-models ~/.luciferai/models

# Verify
ls -la ~/.luciferai/models
```

**Custom models on external drive:**
```bash
# Move custom models
mv models/custom_models /Volumes/ExternalDrive/luciferai-custom-models

# Create symlink
ln -s /Volumes/ExternalDrive/luciferai-custom-models models/custom_models

# Verify
ls -la models/custom_models
```

---

## Troubleshooting

### Model Not Found

**Symptom**: `llm list` doesn't show your custom model

**Solutions**:
1. Check file location: `ls -la models/custom_models/*.gguf`
2. Verify file extension is `.gguf` (not `.ggml` or `.bin`)
3. Restart LuciferAI to rescan: `python3 lucifer.py`
4. Check file permissions: `chmod 644 models/custom_models/*.gguf`

### Disk Space Full

**Symptom**: "No space left on device" errors

**Solutions**:
1. Check space: `df -h`
2. Remove unused models: `ollama rm <model-name>`
3. Clear image cache: `rm -rf ~/.luciferai/images/*`
4. Move models to external drive (see Symlinks section)

### Models Load Slowly

**Symptom**: Long delays when switching models

**Causes**:
- Models on external drive (slower I/O)
- Network-mounted storage (NFS/SMB)
- Insufficient RAM causing swap usage

**Solutions**:
- Move models to local SSD
- Use smaller quantization (Q4_K_M instead of Q8_0)
- Add more RAM
- Close other applications

---

## See Also

- [CUSTOM_MODELS.md](CUSTOM_MODELS.md) - Adding custom GGUF models
- [CUSTOM_INTEGRATIONS.md](CUSTOM_INTEGRATIONS.md) - Image generation and external services
- [MODEL_TIERS.md](MODEL_TIERS.md) - Model installation guide
- [TIER_SYSTEM.md](TIER_SYSTEM.md) - Understanding tier system

---

**Questions?** Use the help command: `python3 lucifer.py -c "help"`
