# Training a Custom Tier 1 LLM From Scratch for LuciferAI

## Overview

This guide walks through training your own Language Model from scratch, specifically optimized for LuciferAI's natural language command parsing, code generation, and error fixing capabilities.

**Target:** Tier 1 Model (3-8B parameters)  
**Focus:** Command understanding, code generation, error fixing  
**Integration:** GGUF format for llamafile backend  
**Testing:** Against existing Tier 1 models (Llama3.2, Gemma2, Phi-3)

---

## Table of Contents

1. [Understanding LuciferAI's Requirements](#understanding-luciferais-requirements)
2. [Architecture Selection](#architecture-selection)
3. [Training Data Preparation](#training-data-preparation)
4. [Training Infrastructure](#training-infrastructure)
5. [Training Process](#training-process)
6. [Model Evaluation](#model-evaluation)
7. [Quantization & Conversion](#quantization--conversion)
8. [Integration Testing](#integration-testing)
9. [Performance Benchmarking](#performance-benchmarking)
10. [Fine-Tuning for LuciferAI](#fine-tuning-for-luciferai)

---

## Understanding LuciferAI's Requirements

### Core Capabilities

LuciferAI models need to excel at:

1. **Natural Language Command Parsing**
   - Intent extraction (watch, run, fix, create, list, search, move)
   - File path fuzzy matching
   - Action type classification (autofix, watch, suggest)

2. **Code Generation**
   - Python scripts with proper error handling
   - Shell commands (bash/zsh compatible)
   - Multi-step task planning

3. **Error Understanding & Fixing**
   - Parse Python tracebacks
   - Identify missing imports, syntax errors, type errors
   - Generate minimal fixes (just the import, not full rewrites)

4. **Context Awareness**
   - Remember previous commands (200 message history)
   - File system context
   - Multi-step debugging workflows

### System Architecture

```
User Input
    ‚Üì
NLP Parser (core/nlp_parser.py)
    ‚Üì
Intent Extraction ‚Üí JSON structured response
    ‚Üì
Enhanced Agent (core/enhanced_agent.py)
    ‚Üì
LLM Backend (core/llm_backend.py)
    ‚Üì
llamafile_agent.py ‚Üí GGUF Model
```

### Expected Input/Output Format

**Input Example:**
```python
{
  "role": "system",
  "content": "You are a command parser for LuciferAI..."
},
{
  "role": "user", 
  "content": "watch my desktop fan terminal file"
}
```

**Expected Output (JSON):**
```json
{
  "intent": "watch",
  "confidence": 0.9,
  "file_hints": ["desktop", "fan", "terminal", "file"],
  "action_type": "watch",
  "reasoning": "User wants to monitor a file for errors"
}
```

---

## Architecture Selection

### Recommended Base Architectures for Tier 1

1. **Llama Architecture** (Preferred)
   - Proven performance
   - Strong code capabilities
   - Good instruction following
   - Size: 3B or 7B parameters

2. **Phi Architecture**
   - Microsoft's efficient design
   - Excellent size/performance ratio
   - Strong reasoning
   - Size: 2.7B parameters

3. **Qwen Architecture**
   - Multilingual support
   - Good coding abilities
   - Modern architecture
   - Size: 7B parameters

### Parameter Budget for Tier 1

- **Minimum:** 2.7B parameters (Phi-2 size)
- **Optimal:** 3-7B parameters
- **Maximum:** 8B parameters
- **Target Quantized Size:** 1.5-4GB (Q4_K_M)

### Why Not Train from Random Init?

**Don't:** Train from scratch (random weights)  
**Do:** Fine-tune an existing instruct model

**Reasoning:**
- Pre-trained models already understand language
- Fine-tuning is 100x cheaper than pretraining
- You need ~10B tokens for pretraining vs ~100M for fine-tuning
- LuciferAI needs specific skills, not general knowledge

---

## Training Data Preparation

### Data Collection Strategy

#### 1. Command Parsing Dataset (High Priority)

Collect 10,000+ examples of:

```jsonl
{"messages": [
  {"role": "system", "content": "You are a command parser for LuciferAI. Parse user commands into structured JSON..."},
  {"role": "user", "content": "watch the test script"},
  {"role": "assistant", "content": "{\"intent\": \"watch\", \"confidence\": 0.95, \"file_hints\": [\"test\", \"script\"], \"action_type\": \"watch\", \"reasoning\": \"User wants daemon mode on test script\"}"}
]}
{"messages": [
  {"role": "system", "content": "You are a command parser for LuciferAI..."},
  {"role": "user", "content": "fix broken_code.py"},
  {"role": "assistant", "content": "{\"intent\": \"fix\", \"confidence\": 1.0, \"file_hints\": [\"broken_code.py\"], \"action_type\": \"autofix\", \"reasoning\": \"User wants to fix errors in file\"}"}
]}
```

**Sources:**
- LuciferAI command logs (if available)
- Synthetic data generation with GPT-4
- Manual creation of edge cases
- User submitted commands

#### 2. Code Generation Dataset

Collect 5,000+ examples of:

```jsonl
{"messages": [
  {"role": "user", "content": "create a python script that opens the browser"},
  {"role": "assistant", "content": "```python\n#!/usr/bin/env python3\nimport webbrowser\nimport sys\n\ntry:\n    webbrowser.open('https://www.google.com')\n    print('‚úÖ Browser opened successfully')\nexcept Exception as e:\n    print(f'‚ùå Error: {e}')\n    sys.exit(1)\n```"}
]}
```

**Key Patterns:**
- File operations (read, write, move, copy)
- Network operations (HTTP requests, API calls)
- System commands (process management, file search)
- Error handling (try/except blocks)
- Proper shebang and imports

#### 3. Error Fixing Dataset (Critical)

Collect 8,000+ examples of:

```jsonl
{"messages": [
  {"role": "user", "content": "Fix this error:\n```python\ndata = json.dumps({'x': 1})\n```\nError: NameError: name 'json' is not defined"},
  {"role": "assistant", "content": "Add import at top:\n```python\nimport json\n```"}
]}
{"messages": [
  {"role": "user", "content": "Fix: TypeError: unsupported operand type(s) for +: 'int' and 'str'\nCode: result = 5 + '10'"},
  {"role": "assistant", "content": "Convert string to int:\n```python\nresult = 5 + int('10')\n```"}
]}
```

**Error Types to Cover:**
- NameError (missing imports, undefined variables)
- TypeError (type mismatches)
- AttributeError (missing methods)
- SyntaxError (parsing errors)
- ImportError (module issues)
- FileNotFoundError (path issues)
- ValueError (invalid values)

#### 4. Context Tracking Dataset

Examples with conversation history:

```jsonl
{"messages": [
  {"role": "user", "content": "show me test.py"},
  {"role": "assistant", "content": "```python\nprint('hello')\n```"},
  {"role": "user", "content": "fix the error in it"},
  {"role": "assistant", "content": "I see the file test.py. What error are you experiencing?"}
]}
```

### Dataset Structure

```
training_data/
‚îú‚îÄ‚îÄ command_parsing/
‚îÇ   ‚îú‚îÄ‚îÄ watch_commands.jsonl (1,500 examples)
‚îÇ   ‚îú‚îÄ‚îÄ fix_commands.jsonl (1,500 examples)
‚îÇ   ‚îú‚îÄ‚îÄ run_commands.jsonl (1,200 examples)
‚îÇ   ‚îú‚îÄ‚îÄ create_commands.jsonl (1,500 examples)
‚îÇ   ‚îú‚îÄ‚îÄ list_commands.jsonl (1,000 examples)
‚îÇ   ‚îú‚îÄ‚îÄ search_commands.jsonl (1,200 examples)
‚îÇ   ‚îú‚îÄ‚îÄ move_commands.jsonl (1,000 examples)
‚îÇ   ‚îî‚îÄ‚îÄ edge_cases.jsonl (600 examples)
‚îú‚îÄ‚îÄ code_generation/
‚îÇ   ‚îú‚îÄ‚îÄ python_scripts.jsonl (2,500 examples)
‚îÇ   ‚îú‚îÄ‚îÄ bash_scripts.jsonl (1,500 examples)
‚îÇ   ‚îî‚îÄ‚îÄ error_handling.jsonl (1,000 examples)
‚îú‚îÄ‚îÄ error_fixing/
‚îÇ   ‚îú‚îÄ‚îÄ name_errors.jsonl (2,000 examples)
‚îÇ   ‚îú‚îÄ‚îÄ type_errors.jsonl (1,500 examples)
‚îÇ   ‚îú‚îÄ‚îÄ syntax_errors.jsonl (1,500 examples)
‚îÇ   ‚îú‚îÄ‚îÄ import_errors.jsonl (1,500 examples)
‚îÇ   ‚îú‚îÄ‚îÄ attribute_errors.jsonl (1,000 examples)
‚îÇ   ‚îî‚îÄ‚îÄ misc_errors.jsonl (500 examples)
‚îî‚îÄ‚îÄ context_tracking/
    ‚îú‚îÄ‚îÄ multi_turn.jsonl (2,000 examples)
    ‚îî‚îÄ‚îÄ file_references.jsonl (1,000 examples)
```

**Total:** ~30,000 training examples

### Data Quality Checklist

‚úÖ **Diversity:** Multiple ways to express same intent  
‚úÖ **Realism:** Real command patterns users actually type  
‚úÖ **Edge Cases:** Typos, ambiguous commands, partial info  
‚úÖ **Balance:** Equal representation across intents  
‚úÖ **Format:** Consistent JSON structure  
‚úÖ **Validation:** No hallucinations, accurate fixes

### Generating Synthetic Data

Use this script to generate training data:

```python
#!/usr/bin/env python3
"""Generate synthetic LuciferAI training data"""
import json
import random

# Templates for command parsing
WATCH_TEMPLATES = [
    "watch {file}",
    "monitor {file}",
    "daemon watch {file}",
    "can you watch {file}",
    "start watching {file}",
    "keep an eye on {file}",
]

FILES = [
    "test.py", "main.py", "script.sh", "daemon.py", 
    "server.js", "app.ts", "index.html", "config.json"
]

def generate_watch_commands(count=1500):
    """Generate watch command examples"""
    examples = []
    for _ in range(count):
        template = random.choice(WATCH_TEMPLATES)
        file = random.choice(FILES)
        user_input = template.format(file=file)
        
        example = {
            "messages": [
                {"role": "system", "content": "You are a command parser for LuciferAI. Parse user commands into structured JSON..."},
                {"role": "user", "content": user_input},
                {"role": "assistant", "content": json.dumps({
                    "intent": "watch",
                    "confidence": 0.9 + random.random() * 0.1,
                    "file_hints": [file],
                    "action_type": "watch",
                    "reasoning": f"User wants to monitor {file} for errors"
                })}
            ]
        }
        examples.append(example)
    
    return examples

# Generate and save
examples = generate_watch_commands()
with open('watch_commands.jsonl', 'w') as f:
    for ex in examples:
        f.write(json.dumps(ex) + '\n')
```

---

## Training Infrastructure

### Hardware Requirements

**Minimum (for 3B model):**
- GPU: 1x RTX 3090 (24GB VRAM) or better
- RAM: 64GB system RAM
- Storage: 500GB NVMe SSD
- Time: ~24-48 hours

**Recommended (for 7B model):**
- GPU: 2x RTX 4090 (48GB total VRAM) or A100 (40GB)
- RAM: 128GB system RAM
- Storage: 1TB NVMe SSD
- Time: ~48-72 hours

**Cloud Options:**
- **Lambda Labs:** $1.10/hr for A100 (40GB)
- **RunPod:** $0.80/hr for RTX 4090
- **Google Colab Pro+:** $50/month (limited)

### Software Stack

```bash
# Create training environment
conda create -n llm-training python=3.10
conda activate llm-training

# Install PyTorch with CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install training framework (choose one)

# Option A: Axolotl (Recommended - easy fine-tuning)
git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl
pip install -e '.[flash-attn,deepspeed]'

# Option B: HuggingFace TRL (More control)
pip install trl transformers datasets peft accelerate bitsandbytes

# Install utilities
pip install wandb tensorboard
```

---

## Training Process

### Step 1: Choose Base Model

Download a pre-trained instruct model:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Option A: Llama 3.2 3B (Tier 1 sweet spot)
model_name = "meta-llama/Llama-3.2-3B-Instruct"

# Option B: Phi-2 (Smaller, efficient)
# model_name = "microsoft/phi-2"

# Option C: Qwen 2.5 7B (Larger, more capable)
# model_name = "Qwen/Qwen2.5-7B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto"
)
```

### Step 2: Prepare Training Configuration

Create `luciferai_finetune.yml`:

```yaml
base_model: meta-llama/Llama-3.2-3B-Instruct
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer

# Training data
datasets:
  - path: training_data/command_parsing/
    type: chat_template
    field: messages
  - path: training_data/code_generation/
    type: chat_template
    field: messages
  - path: training_data/error_fixing/
    type: chat_template
    field: messages
  - path: training_data/context_tracking/
    type: chat_template
    field: messages

# LoRA configuration (efficient fine-tuning)
adapter: lora
lora_r: 32
lora_alpha: 64
lora_dropout: 0.05
lora_target_linear: true

# Training hyperparameters
sequence_len: 2048
sample_packing: true
pad_to_sequence_len: true

micro_batch_size: 2
gradient_accumulation_steps: 8
num_epochs: 3
learning_rate: 2e-5
lr_scheduler: cosine
warmup_steps: 100

# Optimization
gradient_checkpointing: true
fp16: false
bf16: true
tf32: true
flash_attention: true

# Evaluation
val_set_size: 0.05
eval_steps: 50
save_steps: 100
logging_steps: 10

# Output
output_dir: ./luciferai-llama3.2-3b-tier1
hub_model_id: username/luciferai-llama3.2-3b-tier1

# Monitoring
wandb_project: luciferai-training
wandb_entity: your-username
```

### Step 3: Start Training

```bash
# Using Axolotl
accelerate launch -m axolotl.cli.train luciferai_finetune.yml

# Monitor with WandB
# https://wandb.ai/your-username/luciferai-training

# Training will take 24-72 hours depending on hardware
```

### Step 4: Merge LoRA Weights

After training completes:

```bash
# Merge LoRA adapter with base model
python -m axolotl.cli.merge_lora \
  luciferai_finetune.yml \
  --lora_model_dir ./luciferai-llama3.2-3b-tier1

# Output: merged model in ./luciferai-llama3.2-3b-tier1-merged/
```

---

## Model Evaluation

### Pre-Integration Testing

Test your model BEFORE converting to GGUF:

```python
#!/usr/bin/env python3
"""Test custom model capabilities"""
from transformers import AutoModelForCausalLM, AutoTokenizer
import json

# Load your custom model
model_path = "./luciferai-llama3.2-3b-tier1-merged"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto")

def test_command_parsing():
    """Test command parsing capability"""
    system_prompt = "You are a command parser for LuciferAI. Parse user commands into structured JSON..."
    user_input = "watch my test script"
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]
    
    inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")
    outputs = model.generate(inputs, max_new_tokens=256, temperature=0.3)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print("Command Parsing Test:")
    print(f"Input: {user_input}")
    print(f"Output: {response}")
    
    # Validate JSON structure
    try:
        parsed = json.loads(response.split("assistant")[-1].strip())
        assert "intent" in parsed
        assert "confidence" in parsed
        print("‚úÖ Valid JSON structure")
    except:
        print("‚ùå Invalid JSON - needs more training")

def test_code_generation():
    """Test code generation capability"""
    prompt = "Create a Python script that reads a JSON file"
    
    messages = [{"role": "user", "content": prompt}]
    inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")
    outputs = model.generate(inputs, max_new_tokens=512, temperature=0.7)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print("\nCode Generation Test:")
    print(f"Input: {prompt}")
    print(f"Output: {response}")
    
    # Check for key elements
    if "import json" in response and "open(" in response:
        print("‚úÖ Contains proper imports and file handling")
    else:
        print("‚ùå Missing key elements")

def test_error_fixing():
    """Test error fixing capability"""
    prompt = """Fix this error:
```python
data = json.dumps({'x': 1})
```
Error: NameError: name 'json' is not defined"""
    
    messages = [{"role": "user", "content": prompt}]
    inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")
    outputs = model.generate(inputs, max_new_tokens=256, temperature=0.3)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print("\nError Fixing Test:")
    print(f"Output: {response}")
    
    if "import json" in response and len(response) < 200:
        print("‚úÖ Provides minimal, correct fix")
    else:
        print("‚ùå Fix is too verbose or incorrect")

# Run tests
test_command_parsing()
test_code_generation()
test_error_fixing()
```

### Quality Benchmarks

Your model should achieve:

- **Command Parsing Accuracy:** >90% correct intent extraction
- **JSON Format Compliance:** >95% valid JSON responses
- **Code Generation Quality:** Executable code >85% of the time
- **Error Fix Precision:** Minimal fixes (not full rewrites)
- **Context Retention:** Remember last 5-10 messages

---

## Quantization & Conversion

### Step 1: Convert to GGUF

```bash
# Clone llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# Convert your model to F16 GGUF
python3 convert.py \
  /path/to/luciferai-llama3.2-3b-tier1-merged \
  --outfile luciferai-tier1-f16.gguf \
  --outtype f16
```

### Step 2: Quantize to Q4_K_M

```bash
# Create Q4_K_M quantization (recommended for Tier 1)
./quantize luciferai-tier1-f16.gguf luciferai-tier1-q4_k_m.gguf Q4_K_M

# Optional: Create multiple quantizations for comparison
./quantize luciferai-tier1-f16.gguf luciferai-tier1-q5_k_m.gguf Q5_K_M
./quantize luciferai-tier1-f16.gguf luciferai-tier1-q3_k_m.gguf Q3_K_M
```

### Step 3: Test GGUF Model

```bash
# Test with llamafile
wget https://github.com/Mozilla-Ocho/llamafile/releases/latest/download/llamafile
chmod +x llamafile

# Interactive test
./llamafile -m luciferai-tier1-q4_k_m.gguf -i

# Try these prompts:
# 1. "watch test.py"
# 2. "create a python script that lists files"
# 3. "Fix: NameError: name 'json' is not defined"
```

---

## Integration Testing

### Step 1: Add Model to LuciferAI

```bash
# Copy to custom models directory
cp luciferai-tier1-q4_k_m.gguf \
   ~/Desktop/Projects/LuciferAI_Local/models/custom_models/
```

### Step 2: Configure Tier System

Edit `core/model_tiers.py`:

```python
MODEL_TIERS = {
    # ... existing entries ...
    
    # Your custom Tier 1 model
    'luciferai-tier1': 1,
    'custom-tier1': 1,
    'my-tier1-model': 1,
}
```

Edit `core/model_files_map.py`:

```python
MODEL_FILES = {
    # ... existing entries ...
    
    # Your custom model
    'luciferai-tier1': 'luciferai-tier1-q4_k_m.gguf',
    'custom-tier1': 'luciferai-tier1-q4_k_m.gguf',
}

MODEL_ALIASES = {
    # ... existing entries ...
    
    'custom-tier1': 'luciferai-tier1',
}
```

### Step 3: Enable and Test

```bash
cd ~/Desktop/Projects/LuciferAI_Local

# Start LuciferAI
python3 lucifer.py

# Enable your model
llm enable luciferai-tier1

# Verify it's active
llm list
```

### Step 4: Run Integration Tests

```bash
# Test command parsing
echo "Test 1: Command Parsing"
python3 lucifer.py -c "watch my test script"

# Test code generation
echo "Test 2: Code Generation"
python3 lucifer.py -c "create a python script that opens the browser and save it to desktop"

# Test error fixing
echo "Test 3: Error Fixing"
# First create a broken script
cat > test_broken.py << 'EOF'
data = json.dumps({'x': 1})
print(data)
EOF
python3 lucifer.py -c "fix test_broken.py"

# Test context awareness
echo "Test 4: Context Awareness"
python3 lucifer.py -c "show me test.py"
python3 lucifer.py -c "fix the error in it"
```

---

## Performance Benchmarking

### Benchmark Suite

Run LuciferAI's tier test suite:

```bash
# Compare your model against existing Tier 1 models
./test_all_tiers.sh
```

This will test:
1. Command parsing accuracy
2. Code generation quality
3. Error fixing precision
4. Response speed
5. Memory usage

### Key Metrics

Track these metrics:

```python
#!/usr/bin/env python3
"""Benchmark custom model vs existing Tier 1"""
import time
import json
from pathlib import Path
from core.enhanced_agent import EnhancedLuciferAgent

def benchmark_model(model_name, test_commands):
    """Benchmark model performance"""
    agent = EnhancedLuciferAgent(model=model_name)
    
    results = {
        'model': model_name,
        'command_parsing_success': 0,
        'code_gen_success': 0,
        'error_fix_success': 0,
        'avg_response_time': 0,
        'json_valid_rate': 0,
    }
    
    times = []
    json_valid = 0
    
    for cmd in test_commands['parsing']:
        start = time.time()
        response = agent.process_request(cmd)
        elapsed = time.time() - start
        times.append(elapsed)
        
        # Check if response is valid JSON
        try:
            json.loads(response)
            json_valid += 1
        except:
            pass
    
    results['avg_response_time'] = sum(times) / len(times)
    results['json_valid_rate'] = json_valid / len(test_commands['parsing'])
    
    return results

# Test suite
test_commands = {
    'parsing': [
        "watch test.py",
        "fix broken_code.py",
        "create a script",
        "run the daemon",
        "list files in src",
    ],
    'code_gen': [
        "create a python script that opens browser",
        "write a bash script to backup files",
        "make a json config file",
    ],
    'error_fix': [
        "Fix: NameError: name 'json' is not defined",
        "Fix: TypeError: unsupported operand type",
        "Fix: ImportError: No module named 'requests'",
    ]
}

# Benchmark your model vs existing Tier 1
models = ['luciferai-tier1', 'llama3.2', 'gemma2', 'phi-3']

for model in models:
    print(f"\nBenchmarking {model}...")
    results = benchmark_model(model, test_commands)
    print(json.dumps(results, indent=2))
```

### Success Criteria

Your custom model should match or exceed existing Tier 1 models:

| Metric | Target | Llama3.2 | Gemma2 | Your Model |
|--------|--------|----------|--------|------------|
| Command Parsing | >90% | 92% | 89% | ? |
| JSON Valid Rate | >95% | 97% | 94% | ? |
| Code Execution | >85% | 87% | 83% | ? |
| Avg Response Time | <3s | 2.1s | 2.5s | ? |
| Fix Precision | Minimal | Good | Good | ? |

---

## Fine-Tuning for LuciferAI

### Iterative Improvement

After initial integration, collect real usage data:

```bash
# Enable logging
export LUCIFER_LOG_LEVEL=debug

# Use LuciferAI normally
python3 lucifer.py

# Check logs
tail -f ~/.luciferai/logs/session_*.json
```

### Collect Failure Cases

```python
#!/usr/bin/env python3
"""Extract failing examples for retraining"""
import json
from pathlib import Path

logs_dir = Path.home() / ".luciferai" / "logs"

failures = []

for log_file in logs_dir.glob("session_*.json"):
    with open(log_file) as f:
        for line in f:
            try:
                entry = json.loads(line)
                
                # Check for parsing failures
                if "error" in entry or "invalid" in entry.get("response", "").lower():
                    failures.append({
                        "input": entry.get("user_input"),
                        "output": entry.get("response"),
                        "expected": "correct response",
                        "issue": "needs improvement"
                    })
            except:
                pass

# Save failures for retraining
with open("failures_to_fix.jsonl", "w") as f:
    for failure in failures:
        f.write(json.dumps(failure) + "\n")

print(f"Collected {len(failures)} failure cases")
```

### Retrain with Corrections

1. Manually correct the failures
2. Add to training dataset
3. Run another training epoch
4. Test improvements

---

## Troubleshooting

### Model Gives Invalid JSON

**Problem:** Responses aren't valid JSON structures

**Solution:**
- Add more JSON formatting examples to training data
- Use structured output forcing during inference
- Increase temperature=0.1 for more deterministic outputs

### Model is Too Verbose

**Problem:** Fixes entire scripts instead of minimal changes

**Solution:**
- Add examples of minimal fixes to training data
- Add explicit instructions: "Provide ONLY the fix, not full code"
- Fine-tune with DPO (Direct Preference Optimization)

### Poor Context Retention

**Problem:** Doesn't remember previous messages

**Solution:**
- Increase training sequence length to 4096
- Add more multi-turn conversation examples
- Test with longer context during inference

### Slow Inference Speed

**Problem:** Takes >5 seconds per response

**Solution:**
- Use more aggressive quantization (Q3_K_M)
- Reduce max_tokens limit
- Use smaller base model (3B instead of 7B)

---

## Production Checklist

Before deploying your custom model:

- [ ] Passes all integration tests
- [ ] JSON format compliance >95%
- [ ] Command parsing accuracy >90%
- [ ] Response time <3 seconds
- [ ] Code execution success >85%
- [ ] Proper error handling
- [ ] Context awareness verified
- [ ] Benchmarked against existing Tier 1 models
- [ ] Tested with edge cases
- [ ] Quantized to Q4_K_M successfully
- [ ] Works with llamafile backend
- [ ] Documentation updated
- [ ] Version tagged and saved

---

## Example: Complete Training Pipeline

Here's the full pipeline in one script:

```bash
#!/bin/bash
# Complete pipeline: Data ‚Üí Train ‚Üí Convert ‚Üí Test ‚Üí Deploy

set -e

PROJECT_ROOT="$HOME/llm-training"
cd "$PROJECT_ROOT"

# Step 1: Generate training data
echo "Generating training data..."
python3 generate_training_data.py

# Step 2: Train model
echo "Training model (this will take 24-72 hours)..."
accelerate launch -m axolotl.cli.train luciferai_finetune.yml

# Step 3: Merge LoRA
echo "Merging LoRA weights..."
python -m axolotl.cli.merge_lora luciferai_finetune.yml \
  --lora_model_dir ./luciferai-llama3.2-3b-tier1

# Step 4: Test before conversion
echo "Testing PyTorch model..."
python3 test_model.py

# Step 5: Convert to GGUF
echo "Converting to GGUF..."
cd llama.cpp
python3 convert.py \
  "$PROJECT_ROOT/luciferai-llama3.2-3b-tier1-merged" \
  --outfile "$PROJECT_ROOT/luciferai-tier1-f16.gguf" \
  --outtype f16

# Step 6: Quantize
echo "Quantizing to Q4_K_M..."
./quantize \
  "$PROJECT_ROOT/luciferai-tier1-f16.gguf" \
  "$PROJECT_ROOT/luciferai-tier1-q4_k_m.gguf" \
  Q4_K_M

# Step 7: Test GGUF
echo "Testing GGUF model..."
./llamafile -m "$PROJECT_ROOT/luciferai-tier1-q4_k_m.gguf" \
  --prompt "watch test.py" \
  --temp 0.3 \
  --n-predict 256

# Step 8: Deploy to LuciferAI
echo "Deploying to LuciferAI..."
cp "$PROJECT_ROOT/luciferai-tier1-q4_k_m.gguf" \
   ~/Desktop/Projects/LuciferAI_Local/models/custom_models/

# Step 9: Run integration tests
cd ~/Desktop/Projects/LuciferAI_Local
./test_all_tiers.sh

echo "‚úÖ Training pipeline complete!"
echo "Your custom Tier 1 model is ready to use"
```

---

## Resources

### Training Frameworks
- [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) - Easy fine-tuning
- [HuggingFace TRL](https://github.com/huggingface/trl) - Advanced training
- [LitGPT](https://github.com/Lightning-AI/litgpt) - Lightning-fast training

### Datasets
- [Code Alpaca](https://github.com/sahil280114/codealpaca) - Code generation
- [WizardCoder](https://github.com/nlpxucan/WizardLM) - Code instructions
- [Python Code Instructions](https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca)

### Model Architectures
- [Llama](https://github.com/facebookresearch/llama) - Meta's models
- [Phi](https://huggingface.co/microsoft/phi-2) - Microsoft's efficient models
- [Qwen](https://github.com/QwenLM/Qwen) - Alibaba's multilingual models

### Tools
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - GGUF conversion
- [llamafile](https://github.com/Mozilla-Ocho/llamafile) - Testing
- [WandB](https://wandb.ai) - Training monitoring

---

## Conclusion

Training a custom LLM from scratch for LuciferAI requires:

1. ‚úÖ Understanding LuciferAI's specific needs (command parsing, code gen, error fixing)
2. ‚úÖ Collecting 30K+ high-quality training examples
3. ‚úÖ Fine-tuning an existing instruct model (don't train from random init)
4. ‚úÖ Testing thoroughly before quantization
5. ‚úÖ Converting to GGUF and quantizing to Q4_K_M
6. ‚úÖ Integration testing against existing Tier 1 models
7. ‚úÖ Iterative improvement based on real usage

**Expected Results:**
- A custom 3-7B parameter model
- ~2-4GB quantized size
- Specialized for LuciferAI's workflows
- Competitive with Llama3.2, Gemma2, Phi-3
- Optimized for command parsing and code generation

Good luck building your custom Tier 1 model! üöÄ
